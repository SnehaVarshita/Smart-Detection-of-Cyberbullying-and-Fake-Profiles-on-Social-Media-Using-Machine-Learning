{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5198d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a5fa832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47687</th>\n",
       "      <td>Black ppl aren't expected to do anything, depe...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47688</th>\n",
       "      <td>Turner did not withhold his disappointment. Tu...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47689</th>\n",
       "      <td>I swear to God. This dumb nigger bitch. I have...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47690</th>\n",
       "      <td>Yea fuck you RT @therealexel: IF YOURE A NIGGE...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47691</th>\n",
       "      <td>Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...</td>\n",
       "      <td>ethnicity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47692 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweet_text cyberbullying_type\n",
       "0      In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1      Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2      @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3      @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4      @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying\n",
       "...                                                  ...                ...\n",
       "47687  Black ppl aren't expected to do anything, depe...          ethnicity\n",
       "47688  Turner did not withhold his disappointment. Tu...          ethnicity\n",
       "47689  I swear to God. This dumb nigger bitch. I have...          ethnicity\n",
       "47690  Yea fuck you RT @therealexel: IF YOURE A NIGGE...          ethnicity\n",
       "47691  Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...          ethnicity\n",
       "\n",
       "[47692 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('Files/cyberbullying_tweets.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7f3f24a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['not_cyberbullying', 'gender', 'religion', 'other_cyberbullying',\n",
       "       'age', 'ethnicity'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cyberbullying_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf381a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words from text\n",
    "def clean_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "# cleaning and removing punctuations\n",
    "def clean_puctuations(text):\n",
    "    english_puctuations = string.punctuation\n",
    "    translator =str.maketrans('','', english_puctuations)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# cleaning and removing repeating characters\n",
    "def clean_repeating_characters(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "\n",
    "# cleaning and removing URLs\n",
    "def clean_url(text):\n",
    "    return re.sub(r\"((www.[^s]+)|(http\\S+))\",\"\",text)\n",
    "\n",
    "# cleaning and removing numeric data\n",
    "\n",
    "def clean_Numeric(text):\n",
    "    return re.sub('[0-9]+','',text)\n",
    "\n",
    "#Stemming\n",
    "def text_stemming(text):\n",
    "    st =nltk.PorterStemmer()\n",
    "    text=[st.stem(word) for word in text]\n",
    "    return text\n",
    "\n",
    "# lemmatization\n",
    "def text_lemmatization(text):\n",
    "    lm=nltk.WordNetLemmatizer()\n",
    "    text=[lm.lemmatize(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70738304",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets, type= list(data['tweet_text']), list(data['cyberbullying_type'])\n",
    "labelencoder = LabelEncoder()\n",
    "data['cyberbullying_type_encoded']=labelencoder.fit_transform(data['cyberbullying_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5914ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tweet text to lower case\n",
    "data['tweet_text'] =data['tweet_text'].str.lower()\n",
    "data['tweet_text']=data['tweet_text'].apply(lambda text: clean_stopwords(text))\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x : clean_puctuations(x))\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x :clean_repeating_characters(x))\n",
    "data['tweet_text'] =data['tweet_text'].apply(lambda x :clean_url(x))\n",
    "data['tweet_text']=data['tweet_text'].apply(lambda x: clean_Numeric(x))\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "data['tweet_text']=data['tweet_text'].apply(tokenizer.tokenize)\n",
    "data['tweet_text']=data['tweet_text'].apply(lambda x: text_stemming(x))\n",
    "data['tweet_text']=data['tweet_text'].apply(lambda x: text_lemmatization(x))\n",
    "data['tweet_text'] = data['tweet_text'].apply(lambda x : \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3493c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X= data['tweet_text']\n",
    "y=data['cyberbullying_type_encoded']\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state= 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9488514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of feature words:  308359\n"
     ]
    }
   ],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features= 500000)\n",
    "vectoriser.fit(X_train)\n",
    "print(\"No. of feature words: \",len(vectoriser.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9024cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=vectoriser.transform(X_train)\n",
    "X_test=vectoriser.transform(X_test)\n",
    "\n",
    "context={\"data\":\"Tweet Data Proccessed With NLTK And No. of feature words Found In Tweets: \"+str(len(vectoriser.get_feature_names_out()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d2637bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Accurary:  0.8221973720995247\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "rfacc = accuracy_score(y_test, y_pred)\n",
    "print(\"RandomForest Accurary: \",rfacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6e743ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTree Accurary:  0.7917249091417389\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "dtacc = accuracy_score(y_test, y_pred)\n",
    "print(\"DecisionTree Accurary: \",dtacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d5fd485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accurary:  0.8291864691081913\n"
     ]
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', C=1, random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "svcacc = accuracy_score(y_test, y_pred)\n",
    "print(\"SVM Accurary: \",svcacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "629cc972",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vectoriser, open('Files/CD_vectoriser.pkl', 'wb'))\n",
    "pickle.dump(svc, open('Files/CD_svm.pkl', 'wb'))\n",
    "pickle.dump(labelencoder, open('Files/CD_labelencoder.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
